model:
  hidden_size: 768
  num_attention_heads: 12
  attention_dropout: 0.1
  window_size: 256
  global_tokens: 32
  sparsity_factor: 0.2
  lsh_num_buckets: 32
  lsh_hash_size: 16

chunking:
  min_chunk_size: 512
  max_chunk_size: 2048
  chunk_growth_factor: 1.5
  overlap_size: 128

memory:
  max_memory_size: 1000
  cache_size: 10000
  prefetch_window: 512
  prediction_threshold: 0.7

training:
  batch_size: 4
  learning_rate: 1e-4
  warmup_steps: 1000
  max_steps: 100000
  gradient_accumulation: 1
  weight_decay: 0.01
  max_grad_norm: 1.0
